{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definindo função para gerar o vocabulário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo que a função deve receber como parâmetro o caminho de um arquivo de texto\n",
    "def cria_vocabulario(caminho_texto):\n",
    "    #utilizando metodo 'open' do python para receber o caminho do arquivo passado e lê-lo\n",
    "    with open(caminho_texto, \"r\",encoding=\"utf8\") as f:\n",
    "        artigo = f.read()\n",
    "    #utilizando técnicas de NLP para partir o texto em partes menores chamadas de Tokens, que separam palavras de pontos e espaços\n",
    "    lista_tokens = nltk.tokenize.word_tokenize(artigo)\n",
    "    lista_palavras = []\n",
    "    for token in lista_tokens:\n",
    "        #realizando uma verificação condicional para determinar se o token é alfabético ou não. Se for alfabético será adicionado à lista de palavras.\n",
    "        if token.isalpha():\n",
    "            lista_palavras.append(token)\n",
    "    lista_normalizada = []\n",
    "    for palavra in lista_palavras:\n",
    "        #normalizando capa palavra da lista gerada anteriormente para que sejam transferidas para o formato lowecase\n",
    "        lista_normalizada.append(palavra.lower())\n",
    "    lista_normalizada\n",
    "    #retornando uma lista de palavras normalizadas\n",
    "    return lista_normalizada\n",
    "#com a função criada, podem ser concatenadas várias listas de geradas a partir de diversos fontes de texto para assim gerar um vocabulário mais extenso e mais preciso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenando funções de geração de vocabulário para obter um vocabulário mais extenso.\n",
    "lista_palavras  = cria_vocabulario(\"dados/txt-dados.txt\")\n",
    "lista_palavras += cria_vocabulario(\"dados/txt-artigo-dados.txt\")\n",
    "lista_palavras += cria_vocabulario(\"dados/artigos.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "todas_palavras = lista_palavras\n",
    "#frequencia com que cada palavra da lista de palavras aparece no conjunto\n",
    "frequencia = nltk.FreqDist(todas_palavras)\n",
    "#quantidade total de palavras do conjunto\n",
    "quant_palavras = len(todas_palavras)\n",
    "#definindo que o vocabulário será a lista de palavras, removendo as aparições repetidas das palavras.\n",
    "vocabulario = set(lista_palavras)\n",
    "\n",
    "'''definindo função que calcula a probabilidade da palavra inserida estar correta, dividindo frequencia de surgimento da \n",
    "palavra pela quantidade total de palavras.'''\n",
    "def probabilidade(palavra_gerada):\n",
    "    return frequencia[palavra_gerada]/quant_palavras\n",
    "\n",
    "#definindo função que resolve falta de caracteres na palavra digitada\n",
    "def insere_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    letras = \"abcdefghijklmnopqrstuvwxyzáéíóúâêôàçñãõ\"\n",
    "    #Recebe duas fatias de uma palavra e insere uma letra entre as fatias\n",
    "    for E, D in fatias:\n",
    "        for letra in letras:\n",
    "            novas_palavras.append(E + letra + D)\n",
    "    return novas_palavras\n",
    "\n",
    "#definindo função que resolve caractere incorreto inserido no meio da palavra\n",
    "def deletando_caracteres(fatias):\n",
    "    novas_palavras = []\n",
    "    #para cada letra da palavra, recebe uma duas fatias da palavra e exlui a primeira letra da segunda fatia\n",
    "    for E, D in fatias:\n",
    "        novas_palavras.append(E + D[1:])\n",
    "    return novas_palavras\n",
    "\n",
    "\n",
    "#criando função que resolve problema de palavra escrita com letras erradas\n",
    "def troca_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    letras = \"abcdefghijklmnopqrstuvwxyzáéíóúâêôàçñãõ\"\n",
    "    #remove a primeira letra da segunda fatia e a substitui por outra letra\n",
    "    for E, D in fatias:\n",
    "        for letra in letras:\n",
    "            novas_palavras.append(E + letra + D[1:])\n",
    "    return novas_palavras\n",
    "\n",
    "#criando função que soluciona digitação de letras invertidas\n",
    "def inverte_letras(fatias):\n",
    "    novas_palavras = []\n",
    "    #inverte a primeira letra da segunda fatia pela ultima da primeira fatia\n",
    "    for E, D in fatias:\n",
    "        if len(D) > 1:\n",
    "            novas_palavras.append(E + D[1] + D[0] + D[2:])\n",
    "    return novas_palavras\n",
    "\n",
    "#Gerando possíveis palavras corretas \n",
    "def gerador_palavras(palavra):\n",
    "    fatias = []\n",
    "    #criando fatias a partir de palavra inserida\n",
    "    for i in range(len(palavra)+1):\n",
    "        fatias.append((palavra[:i], palavra[i:]))\n",
    "    #chamando todas as funções anteriores para gerar uma lista com possíveis correções para a palavra inserida\n",
    "    palavras_geradas = insere_letras(fatias)\n",
    "    palavras_geradas += deletando_caracteres(fatias)\n",
    "    palavras_geradas += troca_letras(fatias)\n",
    "    palavras_geradas += inverte_letras(fatias)\n",
    "    return palavras_geradas\n",
    "\n",
    "#definindo o corretor em si\n",
    "def corretor(palavra):\n",
    "    #chamando a função de geração de palavras e atribuindo ela à palavras_geradas\n",
    "    palavras_geradas = gerador_palavras(palavra)\n",
    "    #definindo que a palavra correta será aquela que tiver a maior porcentagem de aparecimento no nosso conjunto de palavras\n",
    "    palavra_correta = max(palavras_geradas, key=probabilidade)\n",
    "    return palavra_correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aqui é realizado um pequeno teste para verificar a acurácia das correções através de uma lista de palavras erradas e seu par correto\n",
    "palavras_teste = pd.read_table(\"dados/palavras.txt\", sep=\" \", names = [\"correta\", \"errada\"], header=0)\n",
    "corretas = palavras_teste[\"correta\"]\n",
    "#definindo palavras erradas\n",
    "erradas = palavras_teste[\"errada\"]\n",
    "lista_corrigidas = []\n",
    "#definindo lista de palavras corrigidas\n",
    "for errada in erradas:\n",
    "    lista_corrigidas.append(corretor(errada))\n",
    "corrigidas = pd.Series(lista_corrigidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definindo função para avaliar acurácia do corretor\n",
    "def avaliador(correta, corrigida):\n",
    "    numero_palavras = len(corrigida)\n",
    "    acertou = 0\n",
    "    desconhecida = 0\n",
    "    for i in range(numero_palavras):\n",
    "        if correta[i] == corrigida[i]:\n",
    "            acertou += 1\n",
    "        if correta[i] not in vocabulario:\n",
    "            desconhecida += 1    \n",
    "    taxa_acerto = round(acertou*100/numero_palavras, 2)\n",
    "    taxa_desconhecida = round(desconhecida*100/numero_palavras, 2)\n",
    "    print(f\"A acurácia do corretor foi de {taxa_acerto}% de {numero_palavras} palavras. Palavras desconhecidas somam {taxa_desconhecida}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A acurácia do corretor foi de 76.22% de 185 palavras, desconhecida é 7.03%\n"
     ]
    }
   ],
   "source": [
    "#chamando função e verificando acurácia do corretor\n",
    "avaliador(corretas, corrigidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vemos então que é possível otimizar o corretor em até 7.03% caso obtenhamos um vocabulário maior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
